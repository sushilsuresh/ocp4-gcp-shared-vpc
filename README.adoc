== Deploy OpenShift 4.x in GCP to a Shared VPC

You need to have an Organisation setup in your GCP account, to be able to setup
and use link:https://cloud.google.com/vpc/docs/shared-vpc[Shared VPC] networks.
do check the linked documentation for the same. This documentation is going to
assume that we start from scratch, with a Google cloud that has a organisation
setup but no projects or shared VPCs. In realily as a consultant, you will be
walking into a environment where all of this prereqs are already in place.

Please go through the link:prereqs/README.adoc[PreReqs Documentation] to setup
your projects and obtain service accounts keys for your projects. I am being
extremely lazy and granting account owner to both the service accounts that are
created in the prereq process. This was a quick weekdend project, that is my
excuse. You would want to review the
link:https://docs.openshift.com/container-platform/4.3/installing/installing_gcp/installing-gcp-user-infra.html#installation-gcp-permissions_installing-gcp-user-infra[Required GCP Permissions]
section in the OpenShift documentation

The aim here is to, try and follow along the steps documented in the OpenShift
documenation for
link:https://docs.openshift.com/container-platform/4.3/installing/installing_gcp/installing-gcp-user-infra.html[Installing on GCP using Deployment Manager Templates]
, aka UPI (User Provisioned Infrastructure)

Fetch the

==== Creating the installation configuration file

We are going to first activate gcloud using the service account key we have
created for the Service Project - which is going to house the OpenShift
installation.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

export GCLOUD_KEYFILE_JSON=keys/${SERVICE_PROJECT}-sa.json
echo GCLOUD_KEYFILE_JSON=${GCLOUD_KEYFILE_JSON}

./openshift-install create install-config --dir=install-dir
----

NOTE: The gcloud auth acitiavate-service-account worked well for gcloud
      commands, however the installer was not using this authenticated session.
      I had to explicity do the `export GCLOUD_KEYFILE_JSON` to get the
      installer to pick up the right gcloud account.

Backup your install-config.yaml file. I made a change to set the worker replica
count to 0. We do not want the installer to spin up worker nodes.

----
compute:
- hyperthreading: Enabled
  name: worker
  platform: {}
  replicas: 0
----

==== Creating the Kubernetes manifest and Ignition config files
===== Create manifests.
----
# ./openshift-install create manifests --dir=install-dir
----
The installer manifests that are generated references the VPC network using
it's name. When you are using a shared VPC network, this is not enough. When
using just the name of the VPC network it is scoped to be within the current
project. However shared VPC networks are present in the Host Project. In order
to reference a shared VPC network you need to provide the network name for the
shared VPC and the host project name as well.

Till the point the installer supports shared VPCs we need to create the hosts
manually.

===== Removing control plane and worker manifest.
----
rm -f install-dir/openshift/99_openshift-cluster-api_master-machines-*.yaml
rm -f install-dir/rm -f openshift/99_openshift-cluster-api_worker-machineset-*.yaml
----
Making sure master nodes are not schedulable.
----
sed -i 's/mastersSchedulable: true/mastersSchedulable: false/g' install-dir/manifests/cluster-scheduler-02-config.yml
----

===== Create the install configs.
----
./openshift-install create ignition-configs --dir=install-dir
----

==== Exporting common variables
===== Extracting the infrastructure name.
----
jq -r .infraID install-dir/metadata.json
----

===== Exporting common variables for Deployment Manager templates.
----
# exported ENV variables from the prereqs setup
export REPO_HOME=`pwd`
export ORG_ID='1234567890'
export HOST_PROJECT='master-project-270422'
export SERVICE_PROJECT='openshift-270422'
export HOST_PROJECT_SA="automation@${HOST_PROJECT}.iam.gserviceaccount.com"
export SERVICE_PROJECT_SA="automation@${SERVICE_PROJECT}.iam.gserviceaccount.com"

# From the OpenShit documentation
export INSTALL_DIR="${REPO_HOME}/install-dir"
export BASE_DOMAIN='xvpc.mock-up.net'
export BASE_DOMAIN_ZONE_NAME='openshift'
export NETWORK_CIDR='10.0.0.0/16'
export INFRA_ID=`jq -r .infraID ${INSTALL_DIR}/metadata.json`
export REGION=`jq -r .gcp.region ${INSTALL_DIR}/metadata.json`
export PROJECT_NAME=`jq -r .gcp.projectID ${INSTALL_DIR}/metadata.json`
export MASTER_SUBNET_CIDR='10.0.0.0/19'
export WORKER_SUBNET_CIDR='10.0.32.0/19'
export CLUSTER_NAME=`jq -r .clusterName ${INSTALL_DIR}/metadata.json`
----

==== Creating VCP in GCP
Generate the VPC deployment templates.
----
ansible-playbook generate-dm-templates.yml -e template_name=vpc
----
Create the vpc deployment.
NOTE: We need to create the VPC in the Host Project.
Make sure you auth to host project before creating the VPC.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${HOST_PROJECT}-sa.json
gcloud config set project ${HOST_PROJECT}

gcloud deployment-manager deployments create ${INFRA_ID}-vpc --config dm-templates/01_vpc.yaml
----
Make sure the VPCs have been created using cloud console.

Share the newly created VPCs with the Service Project.
Google documenation for
link:https://cloud.google.com/vpc/docs/provisioning-shared-vpc[Provisioning Shared VPC]
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${HOST_PROJECT}-sa.json
gcloud config set project ${HOST_PROJECT}
gcloud compute shared-vpc associated-projects add --host-project=${HOST_PROJECT} ${SERVICE_PROJECT}
----
NOTE: may have to do it as the account owner and not the service account.
      Ran into issue running the above command as the service account user.
      Works find using my Account owner user.


After associating the shared VPC to a project, the next step is to share the
2 x subnets we created in the vpc. The subnets are named
${INFRA_ID}-master-subnet and ${INFRA_ID}-worker-subnet. Sharing the
subnet to the Service Project involes creating a subent-policy.json file and
appying it using gcloud. I have an ansible playbook to generate the json file.
----
ansible-playbook generate-subnet-policy.yaml

ls -1 *.json
cluste-4clmg-master-subnet.json
cluste-4clmg-worker-subnet.json
----
Once you have the json file we apply the subnet policies to the shared VPC
subnet using the below commands.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${HOST_PROJECT}-sa.json
gcloud config set project ${HOST_PROJECT}

gcloud compute networks subnets set-iam-policy ${INFRA_ID}-master-subnet ${INFRA_ID}-master-subnet.json --region ${REGION} --project ${HOST_PROJECT}
gcloud compute networks subnets set-iam-policy ${INFRA_ID}-worker-subnet ${INFRA_ID}-worker-subnet.json --region ${REGION} --project ${HOST_PROJECT}
----
The output of the above command will look something like this.
----
Updated IAM policy for subnetwork [cluste-4clmg-worker-subnet].
bindings:
- members:
  - serviceAccount:automation@openshift-270422.iam.gserviceaccount.com
  role: roles/compute.networkAdmin
- members:
  - serviceAccount:automation@openshift-270422.iam.gserviceaccount.com
  role: roles/compute.networkUser
- members:
  - serviceAccount:automation@openshift-270422.iam.gserviceaccount.com
  role: roles/compute.securityAdmin
etag: BwWgYcWXKFc=
version: 1
----
We can debate if all of the above prvilleges are necessary or not. GCP has a
new feature for
link:https://cloud.google.com/iam/docs/managing-conditional-policies[Conditional Policies]
which could be used to further restrict the privileges that are granted. That
is not in scope for this POC.
link:https://cloud.google.com/iam/docs/conditions-overview[Overview of Cloud IAM Conditions]

Verify if the Service Project created for OpenShift can view the shared VPC
subnets.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

# Gather the usable subnet in the service project
gcloud compute networks subnets list-usable --project ${HOST_PROJECT}
             PROJECT                REGION    NETWORK               SUBNET                      RANGE         SECONDARY_RANGES
master-project-270422  us-east4  cluste-4clmg-network  cluste-4clmg-worker-subnet  10.0.32.0/19
master-project-270422  us-east4  cluste-4clmg-network  cluste-4clmg-master-subnet  10.0.0.0/19
----

==== Creating networking and load balancing components in GCP
We gather the full URL for the VPC network.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${HOST_PROJECT}-sa.json
gcloud config set project ${HOST_PROJECT}

# Get cluster netork
export CLUSTER_NETWORK=`gcloud compute networks describe ${INFRA_ID}-network --format json | jq -r .selfLink`
echo CLUSTER_NETWORK=${CLUSTER_NETWORK}
----
NOTE: This step is slightly different to the OpenShift documentation. We need
      login to the Host Porject to list the VPC network. The Service Project is
      not able to list networks for the Host projects unless permission
      *compute.subnetworks.list* is provided for the Host Project.

*Problem 1*: Service Project is not able to fetch the VPC network.


Generate the infra deployment templates.
----
ansible-playbook generate-dm-templates.yml -e template_name=infra
----

Create deployment for infra components.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

gcloud deployment-manager deployments create ${INFRA_ID}-infra --config dm-templates/02_infra.yaml
----

*Problem 2*: Deployment fails because it is not able to create the private
zone. The private zone within the Service Project is not able to see the shared
VPC in the Host Porject to set the visibility. The only way this will work is
if the private zone is created in the Host Project wth visibility to the VPC
network, and the Service Account for the Service Projects is granted
permissions to update it.

*Workaround*: Update the deployment to NOT create the private zone in the
              Service Project. Create a new deployment to create the private
              zone in the Host Porject. Gotcha : need to make sure all the
              zone update are for the private zone are made as the Host Project
              user.

----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

# Delete previously created deployment
gcloud deployment-manager deployments delete ${INFRA_ID}-infra

# push out the new deployment which doesn't creat the private zone
gcloud deployment-manager deployments create ${INFRA_ID}-infra --config dm-templates/02_infra_patched.yaml
----
Now we need to create the DNS zone in the Host project.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${HOST_PROJECT}-sa.json
gcloud config set project ${HOST_PROJECT}

gcloud deployment-manager deployments create ${INFRA_ID}-infra --config dm-templates/02_infra_host_project.yaml
----

===== Adding DNS entries to both zones
First let us gather the IP for the VIP we just ceated.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

export CLUSTER_IP=`gcloud compute addresses describe ${INFRA_ID}-cluster-public-ip --region=${REGION} --format json | jq -r .address`
echo CLUSTER_IP=${CLUSTER_IP=}
----
Add external DNS entries.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

if [ -f transaction.yaml ]; then rm transaction.yaml; fi
gcloud dns record-sets transaction start --zone ${BASE_DOMAIN_ZONE_NAME}
gcloud dns record-sets transaction add ${CLUSTER_IP} --name api.${CLUSTER_NAME}.${BASE_DOMAIN}. --ttl 60 --type A --zone ${BASE_DOMAIN_ZONE_NAME}
gcloud dns record-sets transaction execute --zone ${BASE_DOMAIN_ZONE_NAME}
----

Add internal DNS entires.
NOTE: We need to create the internal DNS entires in the zone that is hosted in
      the Host Project.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${HOST_PROJECT}-sa.json
gcloud config set project ${HOST_PROJECT}

if [ -f transaction.yaml ]; then rm transaction.yaml; fi
gcloud dns record-sets transaction start --zone ${INFRA_ID}-private-zone
gcloud dns record-sets transaction add ${CLUSTER_IP} --name api.${CLUSTER_NAME}.${BASE_DOMAIN}. --ttl 60 --type A --zone ${INFRA_ID}-private-zone
gcloud dns record-sets transaction add ${CLUSTER_IP} --name api-int.${CLUSTER_NAME}.${BASE_DOMAIN}. --ttl 60 --type A --zone ${INFRA_ID}-private-zone
gcloud dns record-sets transaction execute --zone ${INFRA_ID}-private-zone
----

==== Creating firewall rules and IAM roles in GCP
Gather the NAT IPs for both the subnets and export them.
NOTE: We need to run this against the Host Project where the VPC is created.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${HOST_PROJECT}-sa.json
gcloud config set project ${HOST_PROJECT}

export MASTER_NAT_IP=`gcloud compute addresses describe ${INFRA_ID}-master-nat-ip --region ${REGION} --format json | jq -r .address`
export WORKER_NAT_IP=`gcloud compute addresses describe ${INFRA_ID}-worker-nat-ip --region ${REGION} --format json | jq -r .address`

echo MASTER_NAT_IP=${MASTER_NAT_IP}
echo WORKER_NAT_IP=${WORKER_NAT_IP}
----

Generate the sercurity deployment templates.
----
ansible-playbook generate-dm-templates.yml -e template_name=security
----

Create the deployment for security
NOTE: This needs to be created in the Host Porject where the VPC is defined.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${HOST_PROJECT}-sa.json
gcloud config set project ${HOST_PROJECT}

gcloud deployment-manager deployments create ${INFRA_ID}-security --config dm-templates/03_security.yaml
----

===== Create policy role bindings.
I think this needs to go onto the Service project based on the roles being
defined.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

# Creating service account - I had to add this extra
gcloud iam service-accounts create ${INFRA_ID}-m --display-name ${INFRA_ID}-m
gcloud iam service-accounts create ${INFRA_ID}-w --display-name ${INFRA_ID}-w

export MASTER_SA=${INFRA_ID}-m@${PROJECT_NAME}.iam.gserviceaccount.com
gcloud projects add-iam-policy-binding ${PROJECT_NAME} --member "serviceAccount:${MASTER_SA}" --role "roles/compute.instanceAdmin"
gcloud projects add-iam-policy-binding ${PROJECT_NAME} --member "serviceAccount:${MASTER_SA}" --role "roles/compute.networkAdmin"
gcloud projects add-iam-policy-binding ${PROJECT_NAME} --member "serviceAccount:${MASTER_SA}" --role "roles/compute.securityAdmin"
gcloud projects add-iam-policy-binding ${PROJECT_NAME} --member "serviceAccount:${MASTER_SA}" --role "roles/iam.serviceAccountUser"
gcloud projects add-iam-policy-binding ${PROJECT_NAME} --member "serviceAccount:${MASTER_SA}" --role "roles/storage.admin"

export WORKER_SA=${INFRA_ID}-w@${PROJECT_NAME}.iam.gserviceaccount.com
gcloud projects add-iam-policy-binding ${PROJECT_NAME} --member "serviceAccount:${WORKER_SA}" --role "roles/compute.viewer"
gcloud projects add-iam-policy-binding ${PROJECT_NAME} --member "serviceAccount:${WORKER_SA}" --role "roles/storage.admin"
----

NOTE: the service accounts didn't exist is had to creat them before applying
      the roles.
Create service account key and store it locally for use later.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

gcloud iam service-accounts keys create keys/service-account-key.json --iam-account=${MASTER_SA}
----

==== Creating the RHCOS cluster image for the GCP infrastructure
*Problem 3*: The documenation is not at all clear that you need to fetch the
gcloud storage url.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

export IMAGE_SOURCE=`curl https://raw.githubusercontent.com/openshift/installer/release-4.3/data/data/rhcos.json | jq -r .gcp.url`
echo IMAGE_SOURCE=${IMAGE_SOURCE}

gcloud compute images create "${INFRA_ID}-rhcos-image" --source-uri="${IMAGE_SOURCE}"
----

==== Creating the bootstrap machine in GCP
===== Export the following variable to generate the deployment template.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

export CONTROL_SUBNET=`gcloud compute networks subnets describe ${INFRA_ID}-master-subnet --region=${REGION} --project ${HOST_PROJECT} --format json | jq -r .selfLink`
export CLUSTER_IMAGE=`gcloud compute images describe ${INFRA_ID}-rhcos-image --format json | jq -r .selfLink`
export ZONE_0=`gcloud compute regions describe ${REGION} --format=json | jq -r .zones[0] | cut -d "/" -f9`
export ZONE_1=`gcloud compute regions describe ${REGION} --format=json | jq -r .zones[1] | cut -d "/" -f9`
export ZONE_2=`gcloud compute regions describe ${REGION} --format=json | jq -r .zones[2] | cut -d "/" -f9`

echo CONTROL_SUBNET=${CONTROL_SUBNET}
echo CLUSTER_IMAGE=${CLUSTER_IMAGE}
echo ZONE_0=${ZONE_0}
echo ZONE_1=${ZONE_1}
echo ZONE_2=${ZONE_2}
----
NOTE: I had to add the --project ${HOST_PROJECT} for the subnets describe
      command. as those subnets are in the shared VPC project.

===== Create a bucket and upload the bootstrap.ign file.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

gsutil mb gs://${INFRA_ID}-bootstrap-ignition
gsutil cp ${INSTALL_DIR}/bootstrap.ign gs://${INFRA_ID}-bootstrap-ignition/
----

===== Create a signed URL for the bootstrap instance.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

export BOOTSTRAP_IGN=`gsutil signurl -d 1h keys/service-account-key.json gs://${INFRA_ID}-bootstrap-ignition/bootstrap.ign | grep "^gs:" | awk '{print $5}'`
echo BOOTSTRAP_IGN=${BOOTSTRAP_IGN}
----

===== Generate the deployment templates.
----
ansible-playbook generate-dm-templates.yml -e template_name=bootstrap
----

===== Create the bootstrap deployment.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

gcloud deployment-manager deployments create ${INFRA_ID}-bootstrap --config dm-templates/04_bootstrap.yaml
----

*Problem 4*: This fails because the deploymnet template is attempting to update
the private zone which existins int Host Project whic owns the shared VPC.

*Workaround*: Will need to break up the deployment into 2 parts 1 for the
Service Project and one for the Host Project

First let us delete the failed deployment.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

gcloud deployment-manager deployments delete ${INFRA_ID}-bootstrap
----

Deploy the patched deployment template.

----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

gcloud deployment-manager deployments create ${INFRA_ID}-bootstrap --config dm-templates/04_bootstrap_patched.yaml
----

Got this error with persmission issues when running the boostrap template.
The erorr is documented below.
----
{
  "ResourceType": "compute.v1.instance",
  "ResourceErrorCode": "403",
  "ResourceErrorMessage": {
    "code": 403,
    "errors": [
      {
        "domain": "global",
        "message": "Required 'compute.subnetworks.use' permission for 'projects/master-project-270422/regions/us-east4/subnetworks/cluste-4clmg-master-subnet'
",
        "reason": "forbidden"
      },
      {
        "domain": "global",
        "message": "Required 'compute.subnetworks.useExternalIp' permission for 'projects/master-project-270422/regions/us-east4/subnetworks/cluste-4clmg-mast
er-subnet'",
        "reason": "forbidden"
      }
    ],
    "message": "Required 'compute.subnetworks.use' permission for 'projects/master-project-270422/regions/us-east4/subnetworks/cluste-4clmg-master-subnet'",
    "statusMessage": "Forbidden",
    "requestPath": "https://compute.googleapis.com/compute/v1/projects/openshift-270422/zones/us-east4-a/instances",
    "httpMethod": "POST",
    "suggestion": "Consider granting permissions to 697497380588@cloudservices.gserviceaccount.com"
  }
}
----

Looks like the service account used when VMs are created is the default service
accunt. Running `gcloud iam service-accounts list` will give you the email id.

Adding below user  manually for now. Will need to add it to the
subnet-policy.json
697497380588-compute@developer.gserviceaccount.com

Tried giving the abvoe user Compute Network User and Compte Network Admin
permissions on the Shared VPC subnets. NO GO. Still threw error. That is when
I notited the below.

Interestingly the error talks about service account user.
697497380588@cloudservices.gserviceaccount.com

beyond me. ¯\_(ツ)_/¯ . How to extract this user, progamatically ?
Need to reserch.

Now let us deploy the missing bits to the Host Project.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${HOST_PROJECT}-sa.json
gcloud config set project ${HOST_PROJECT}

gcloud deployment-manager deployments create ${INFRA_ID}-bootstrap --config dm-templates/04_bootstrap_host_project.yaml
----

===== Update LB target pools.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

gcloud compute target-pools add-instances ${INFRA_ID}-api-target-pool --instances-zone="${ZONE_0}" --instances=${INFRA_ID}-bootstrap
gcloud compute target-pools add-instances ${INFRA_ID}-ign-target-pool --instances-zone="${ZONE_0}" --instances=${INFRA_ID}-bootstrap
----

==== Creating the control plane machines in GCP

----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

export MASTER_SERVICE_ACCOUNT_EMAIL=`gcloud iam service-accounts list | grep "^${INFRA_ID}-m" | awk '{print $2}'`
echo MASTER_SERVICE_ACCOUNT_EMAIL=${MASTER_SERVICE_ACCOUNT_EMAIL}

export MASTER_IGNITION=`cat ${INSTALL_DIR}/master.ign`
----

*Problem-5*: The documentaion things i have a service account named
${INFRA_ID}-master-node . But i do not. Furhter up the account is referenced as
${INFRA_ID}-m instead . It could be GCP trimming the long account names

*Workaround*: I am just going to use the account - ${INFRA_ID}-m

Now to generate the deployment templates for control_plane.
----
ansible-playbook generate-dm-templates.yml -e template_name=control_plane
----
Create the deployment for control plane nodes.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

gcloud deployment-manager deployments create ${INFRA_ID}-control-plane --config dm-templates/05_control_plane.yaml
----

Create DNS entires for for the master nodes.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

export MASTER0_IP=`gcloud compute instances describe ${INFRA_ID}-m-0 --zone ${ZONE_0} --format json | jq -r .networkInterfaces[0].networkIP`
export MASTER1_IP=`gcloud compute instances describe ${INFRA_ID}-m-1 --zone ${ZONE_1} --format json | jq -r .networkInterfaces[0].networkIP`
export MASTER2_IP=`gcloud compute instances describe ${INFRA_ID}-m-2 --zone ${ZONE_2} --format json | jq -r .networkInterfaces[0].networkIP`

echo MASTER0_IP=${MASTER0_IP}
echo MASTER1_IP=${MASTER1_IP}
echo MASTER2_IP=${MASTER2_IP}

# The private zone is in the Host Project. so we need to switch over.
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${HOST_PROJECT}-sa.json
gcloud config set project ${HOST_PROJECT}

if [ -f transaction.yaml ]; then rm transaction.yaml; fi
gcloud dns record-sets transaction start --zone ${INFRA_ID}-private-zone
gcloud dns record-sets transaction add ${MASTER0_IP} --name etcd-0.${CLUSTER_NAME}.${BASE_DOMAIN}. --ttl 60 --type A --zone ${INFRA_ID}-private-zone
gcloud dns record-sets transaction add ${MASTER1_IP} --name etcd-1.${CLUSTER_NAME}.${BASE_DOMAIN}. --ttl 60 --type A --zone ${INFRA_ID}-private-zone
gcloud dns record-sets transaction add ${MASTER2_IP} --name etcd-2.${CLUSTER_NAME}.${BASE_DOMAIN}. --ttl 60 --type A --zone ${INFRA_ID}-private-zone
gcloud dns record-sets transaction add "0 10 2380 etcd-0.${CLUSTER_NAME}.${BASE_DOMAIN}." "0 10 2380 etcd-1.${CLUSTER_NAME}.${BASE_DOMAIN}." "0 10 2380 etcd-2.${CLUSTER_NAME}.${BASE_DOMAIN}." --name _etcd-server-ssl._tcp.${CLUSTER_NAME}.${BASE_DOMAIN}. --ttl 60 --type SRV --zone ${INFRA_ID}-private-zone
gcloud dns record-sets transaction execute --zone ${INFRA_ID}-private-zone
----

Adding the master nodes to the appropriate target pools.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

gcloud compute target-pools add-instances ${INFRA_ID}-api-target-pool --instances-zone="${ZONE_0}" --instances=${INFRA_ID}-m-0
gcloud compute target-pools add-instances ${INFRA_ID}-api-target-pool --instances-zone="${ZONE_1}" --instances=${INFRA_ID}-m-1
gcloud compute target-pools add-instances ${INFRA_ID}-api-target-pool --instances-zone="${ZONE_2}" --instances=${INFRA_ID}-m-2
gcloud compute target-pools add-instances ${INFRA_ID}-ign-target-pool --instances-zone="${ZONE_0}" --instances=${INFRA_ID}-m-0
gcloud compute target-pools add-instances ${INFRA_ID}-ign-target-pool --instances-zone="${ZONE_1}" --instances=${INFRA_ID}-m-1
gcloud compute target-pools add-instances ${INFRA_ID}-ign-target-pool --instances-zone="${ZONE_2}" --instances=${INFRA_ID}-m-2
----

==== Creating additional worker machines in GCP

----
# Need to switch to Host Project to describe the subnet
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${HOST_PROJECT}-sa.json
gcloud config set project ${HOST_PROJECT}

export COMPUTE_SUBNET=`gcloud compute networks subnets describe ${INFRA_ID}-worker-subnet --region=${REGION} --format json | jq -r .selfLink`


# Need to switch back to Service Project for the service account.
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

export WORKER_SERVICE_ACCOUNT_EMAIL=`gcloud iam service-accounts list | grep "^${INFRA_ID}-w " | awk '{print $2}'`
export WORKER_IGNITION=`cat ${INSTALL_DIR}/worker.ign`

echo COMPUTE_SUBNET=${COMPUTE_SUBNET}
echo WORKER_SERVICE_ACCOUNT_EMAIL=${WORKER_SERVICE_ACCOUNT_EMAIL}
echo WORKER_IGNITION=${WORKER_IGNITION}
----

*Problem-6*: The documentation thinks I have a service account named
${INFRA_ID}-worker-node . But I do not. Furhter up the account is referenced as
${INFRA_ID}-w instead . It could be GCP trimming the long account names. Who
knows. Either way the documentation is wrong.

Generate the worker dm templates.
----
ansible-playbook generate-dm-templates.yml -e template_name=worker
----

Create the worker deployment.
----
gcloud auth activate-service-account --key-file=${REPO_HOME}/keys/${SERVICE_PROJECT}-sa.json
gcloud config set project ${SERVICE_PROJECT}

gcloud deployment-manager deployments create ${INFRA_ID}-worker --config dm-templates/06_worker.yaml
----

*Probelm-7*: The installer tires to spin up 2 x ingress routers. Since the
master nodes are not schedulable, we should be spinning up a bare binumum of
2 worker nodes i would have thought. Sure i can scare the worker node down no
issue.
*Workaround*: Can add additional worker nodes using the dm templates

====  Adding the ingress DNS records

*Problem-7*: Blocked for now.

The cluster doesn't install completely. as it is not able to provisions an
external IP for the ingress controller.

Current state of the cluster.
Just in case the below output is pretty to read.

https://gist.github.com/sushilsuresh/674032900a0c1bf37c70f8fdfdfc2f6a

----
sushil@bastion:~$ oc get clusterversion
NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS
version             False       True          5h1m    Working towards 4.3.1: 99% complete
sushil@bastion:~$
sushil@bastion:~$
sushil@bastion:~$ oc get co
NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                                       Unknown     Unknown       True       4h48m
cloud-credential                           4.3.1     True        False         False      5h1m
cluster-autoscaler                         4.3.1     True        False         False      4h46m
console                                    4.3.1     False       True          False      3h49m
dns                                        4.3.1     True        False         False      4h55m
image-registry                             4.3.1     True        False         False      3h48m
ingress                                    unknown   False       True          True       4h47m
insights                                   4.3.1     True        False         False      4h56m
kube-apiserver                             4.3.1     True        False         False      4h54m
kube-controller-manager                    4.3.1     True        False         False      4h50m
kube-scheduler                             4.3.1     True        False         False      4h54m
machine-api                                4.3.1     True        False         False      4h55m
machine-config                             4.3.1     True        False         False      4h56m
marketplace                                4.3.1     True        False         False      4h47m
monitoring                                           False       True          True       4h42m
network                                    4.3.1     True        False         False      4h57m
node-tuning                                4.3.1     True        False         False      4h48m
openshift-apiserver                        4.3.1     True        False         False      4h49m
openshift-controller-manager               4.3.1     True        False         False      4h55m
openshift-samples                          4.3.1     True        False         False      4h46m
operator-lifecycle-manager                 4.3.1     True        False         False      4h55m
operator-lifecycle-manager-catalog         4.3.1     True        False         False      4h55m
operator-lifecycle-manager-packageserver   4.3.1     True        False         False      4h53m
service-ca                                 4.3.1     True        False         False      4h56m
service-catalog-apiserver                  4.3.1     True        False         False      4h49m
service-catalog-controller-manager         4.3.1     True        False         False      4h48m
storage                                    4.3.1     True        False         False      4h47m
sushil@bastion:~$
sushil@bastion:~$
sushil@bastion:~$
sushil@bastion:~$ oc project openshift-ingress
Now using project "openshift-ingress" on server "https://api.cluster-02.xvpc.mock-up.net:6443".
sushil@bastion:~$
sushil@bastion:~$
sushil@bastion:~$ oc get service router-default
NAME             TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
router-default   LoadBalancer   172.30.34.152   <pending>     80:32383/TCP,443:30310/TCP   4h50m
sushil@bastion:~$
sushil@bastion:~$
sushil@bastion:~$
sushil@bastion:~$ oc get events
LAST SEEN   TYPE      REASON                   OBJECT                                MESSAGE
<unknown>   Warning   FailedScheduling         pod/router-default-84b7fb5456-4x92z   0/4 nodes are available: 1 node(s) didn't match pod affinity/anti-affinity, 1 node(s) didn't satisfy existing pods anti-affinity rules, 3 node(s) didn't match node selector.
2m44s       Normal    EnsuringLoadBalancer     service/router-default                Ensuring load balancer
23m         Warning   SyncLoadBalancerFailed   service/router-default                Error syncing load balancer: failed to ensure load balancer: googleapi: Error 404: The resource 'projects/openshift-270422/global/networks/cluste-2f6gd-network' was not found, notFound
sushil@bastion:~$
----


